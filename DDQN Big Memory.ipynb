{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from collections import deque\n",
    "from itertools import count\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# создаем энвайронмент с игрой\n",
    "env = gym.make('BreakoutDeterministic-v4').unwrapped\n",
    "\n",
    "# настраиваем matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display    \n",
    "print(\"Is python : {}\".format(is_ipython))\n",
    "\n",
    "# выбираем девайс для игры\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device : {}\".format(device))\n",
    "\n",
    "# запоминаем, сколько действий в игре\n",
    "ACTIONS_NUM = env.action_space.n\n",
    "print(\"Number of actions : {}\".format(ACTIONS_NUM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_SIZE = 4\n",
    "STATE_W = 84\n",
    "STATE_H = 84\n",
    "MEMSIZE = 100000\n",
    "\n",
    "# Задание 1. Необходимо реализовать класс для хранения состояния игры. \n",
    "# В качестве последнего мы будем использовать состеканные 4 последовательных кадра игры.\n",
    "# Это необходимо, чтобы агент понимал скорости и ускорения игровых объектов.\n",
    "    \n",
    "class StateHolder:\n",
    "    \n",
    "    def __init__(self, number_screens=4):\n",
    "        self.first_action = True\n",
    "        self.state = torch.ByteTensor(1, 84, 84).to(device)\n",
    "        self.number_screens = number_screens\n",
    "        \n",
    "    def push(self, screen):\n",
    "        new_screen = screen.squeeze(0)\n",
    "        if self.first_action:\n",
    "            self.state[0] = new_screen\n",
    "            for number in range(self.number_screens-1):\n",
    "                self.state = torch.cat((self.state, new_screen), 0)\n",
    "            self.first_action = False\n",
    "        else:\n",
    "            self.state = torch.cat((self.state, new_screen), 0)[1:]\n",
    "    \n",
    "    def get(self):\n",
    "        return self.state.unsqueeze(0)\n",
    "\n",
    "    def reset(self):\n",
    "        self.first_action = True\n",
    "        self.state = torch.ByteTensor(1, 84, 84).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    \"\"\"Replay Memory that stores the last size=1,000,000 transitions\"\"\"\n",
    "    def __init__(self, size=1000000, frame_height=84, frame_width=84, \n",
    "                 agent_history_length=4, batch_size=32):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            size: Integer, Number of stored transitions\n",
    "            frame_height: Integer, Height of a frame of an Atari game\n",
    "            frame_width: Integer, Width of a frame of an Atari game\n",
    "            agent_history_length: Integer, Number of frames stacked together to create a state\n",
    "            batch_size: Integer, Number if transitions returned in a minibatch\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.frame_height = frame_height\n",
    "        self.frame_width = frame_width\n",
    "        self.agent_history_length = agent_history_length\n",
    "        self.batch_size = batch_size\n",
    "        self.count = 0\n",
    "        self.current = 0\n",
    "        \n",
    "        # Pre-allocate memory\n",
    "        self.actions = np.empty(self.size, dtype=np.int32)\n",
    "        self.rewards = np.empty(self.size, dtype=np.float32)\n",
    "        self.frames = np.empty((self.size, self.frame_height, self.frame_width), dtype=np.uint8)\n",
    "        self.terminal_flags = np.empty(self.size, dtype=np.bool)\n",
    "        \n",
    "        # Pre-allocate memory for the states and new_states in a minibatch\n",
    "        self.states = np.empty((self.batch_size, self.agent_history_length, \n",
    "                                self.frame_height, self.frame_width), dtype=np.uint8)\n",
    "        self.new_states = np.empty((self.batch_size, self.agent_history_length, \n",
    "                                    self.frame_height, self.frame_width), dtype=np.uint8)\n",
    "        self.indices = np.empty(self.batch_size, dtype=np.int32)\n",
    "        \n",
    "    def add_experience(self, action, frame, reward, terminal):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            action: An integer between 0 and env.action_space.n - 1 \n",
    "                determining the action the agent perfomed\n",
    "            frame: A (84, 84, 1) frame of an Atari game in grayscale\n",
    "            reward: A float determining the reward the agend received for performing an action\n",
    "            terminal: A bool stating whether the episode terminated\n",
    "        \"\"\"\n",
    "        if frame.shape != (self.frame_height, self.frame_width):\n",
    "            raise ValueError('Dimension of frame is wrong!')\n",
    "        self.actions[self.current] = action\n",
    "        self.frames[self.current, ...] = frame\n",
    "        self.rewards[self.current] = reward\n",
    "        self.terminal_flags[self.current] = terminal\n",
    "        self.count = max(self.count, self.current+1)\n",
    "        self.current = (self.current + 1) % self.size\n",
    "             \n",
    "    def _get_state(self, index):\n",
    "        if self.count is 0:\n",
    "            raise ValueError(\"The replay memory is empty!\")\n",
    "        if index < self.agent_history_length - 1:\n",
    "            raise ValueError(\"Index must be min 3\")\n",
    "        return self.frames[index-self.agent_history_length+1:index+1, ...]\n",
    "        \n",
    "    def _get_valid_indices(self):\n",
    "        for i in range(self.batch_size):\n",
    "            while True:\n",
    "                index = random.randint(self.agent_history_length, self.count - 1)\n",
    "                if index < self.agent_history_length:\n",
    "                    continue\n",
    "                if index >= self.current and index - self.agent_history_length <= self.current:\n",
    "                    continue\n",
    "                if self.terminal_flags[index - self.agent_history_length:index].any():\n",
    "                    continue\n",
    "                break\n",
    "            self.indices[i] = index\n",
    "            \n",
    "    def get_minibatch(self):\n",
    "        \"\"\"\n",
    "        Returns a minibatch of self.batch_size = 32 transitions\n",
    "        \"\"\"\n",
    "        if self.count < self.agent_history_length:\n",
    "            raise ValueError('Not enough memories to get a minibatch')\n",
    "        \n",
    "        self._get_valid_indices()\n",
    "            \n",
    "        for i, idx in enumerate(self.indices):\n",
    "            self.states[i] = self._get_state(idx - 1)\n",
    "            self.new_states[i] = self._get_state(idx)\n",
    "        \n",
    "        return self.states, self.actions[self.indices], self.rewards[self.indices], self.new_states, self.terminal_flags[self.indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задание 2. Собрать архитектуру сети (DQN).\n",
    "# В качестве примера можно использовать сеть вида:\n",
    "# Conv(4->32) -> Conv(32->64) -> Conv(64->64) -> FC(512) -> FC(ACTIONS_NUM)\n",
    "# В качестве функций активации необходимо использовать ReLU(но совершенно не обязательно ими ограничиваться)\n",
    "# Attention : не забудьте правильно инициализировать веса, это важно для данной задачи!\n",
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        nn.init.kaiming_normal_(self.conv1.weight, nonlinearity='relu')\n",
    "        nn.init.constant_(self.conv1.bias, 0)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        nn.init.kaiming_normal_(self.conv2.weight, nonlinearity='relu')\n",
    "        nn.init.constant_(self.conv2.bias, 0)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        nn.init.kaiming_normal_(self.conv3.weight, nonlinearity='relu')\n",
    "        nn.init.constant_(self.conv3.bias, 0)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(64, 1024, kernel_size=7, stride=1)\n",
    "        nn.init.kaiming_normal_(self.conv4.weight, nonlinearity='relu')\n",
    "        nn.init.constant_(self.conv4.bias, 0)\n",
    "        # add comment\n",
    "        self.fc_value = nn.Linear(512, 1)\n",
    "        nn.init.kaiming_normal_(self.fc_value.weight, nonlinearity='relu')\n",
    "        self.fc_advantage = nn.Linear(512, 4)\n",
    "        nn.init.kaiming_normal_(self.fc_advantage.weight, nonlinearity='relu')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x/255 \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        # add comment\n",
    "        x_value = x[:,:512,:,:].view(-1,512)\n",
    "        x_advantage = x[:,512:,:,:].view(-1,512)\n",
    "        x_value = self.fc_value(x_value)\n",
    "        x_advantage = self.fc_advantage(x_advantage)\n",
    "        # add comment\n",
    "        q_value = x_value + x_advantage.sub(torch.mean(x_advantage, 1)[:, None])\n",
    "        return q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тут блок с кодом, генерирующим 1 кадр игры\n",
    "# Обратите внимание, что выходным тензора является torch.ByteTensor со значениями 0-255\n",
    "# Это сделанно намеренно для экономии места(4х экономия по сравнению с FloatTensor)\n",
    "# Подумайте, где и как необходимо совершать преобразование ByteTensort -> FloatTensor, чтобы его можно было подавать в сеть. \n",
    "\n",
    "# Далее стандартный метод для выбора нового действия из лекции\n",
    "\n",
    "policy_net = DuelingDQN().to(device)\n",
    "target_net = DuelingDQN().to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=1e-4)\n",
    "\n",
    "def select_action(state, eps_threshold):\n",
    "    global steps_done    \n",
    "    sample = random.random()\n",
    "    if sample > eps_threshold and state is not None:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state.float()).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(ACTIONS_NUM)]], device=device, dtype=torch.long)\n",
    "\n",
    "mean_size = 100\n",
    "mean_step = 1\n",
    "train_rewards = []\n",
    "\n",
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize( (STATE_W, STATE_H), interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "def get_screen():\n",
    "    screen = env.render(mode='rgb_array')\n",
    "    screen = np.dot(screen[...,:3], [0.299, 0.587, 0.114])\n",
    "    screen = screen[30:195,:]\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.uint8).reshape(screen.shape[0],screen.shape[1],1)\n",
    "    return resize(screen).mul(255).type(torch.ByteTensor).to(device).detach().unsqueeze(0)\n",
    "\n",
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(get_screen().cpu().reshape(-1,84).numpy(),\n",
    "           interpolation='none', cmap = 'gray')\n",
    "plt.title('Example extracted screen')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "train_rewards = []\n",
    "def plot_rewards(rewards = train_rewards, name = \"Train\"):\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    plt.title(name)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(rewards)\n",
    "    # Строим график среднего вознаграждения по 100 последним эпизодам\n",
    "    if len(rewards) > mean_size:\n",
    "        means = np.array([rewards[i:i+mean_size:] for i in range(0, len(rewards) - mean_size, mean_step)]).mean(1)\n",
    "        means = np.concatenate((np.zeros(mean_size - 1), means))\n",
    "        plt.plot(means)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internet\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "\n",
    "def optimize_model():\n",
    "    # выбираем новый батч\n",
    "    state_batch, action_batch, reward_batch, next_state, terminal_flags = memory.get_minibatch()\n",
    "\n",
    "    # Для всех состояний считаем маску не финальнсти и конкантенируем их\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not True,\n",
    "                                          terminal_flags)), device=device, dtype=torch.uint8)\n",
    "\n",
    "    non_final_next_states = torch.Tensor([next_state[i] for i in range(terminal_flags.shape[0])\n",
    "                                                if terminal_flags[i] is not True]).float().to(device) \n",
    "    state_batch = torch.Tensor(state_batch).float().to(device)\n",
    "    action_batch = torch.Tensor(action_batch).type(torch.LongTensor).to(device).view(-1,1)\n",
    "    reward_batch = torch.Tensor(reward_batch).to(device)\n",
    "    \n",
    "    # Считаем Q(s_t, a) - модель дает Q(s_t), затем мы выбираем\n",
    "    # колонки, которые соответствуют нашим действиям на щаге\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Подсчитываем ценность состяония V(s_{t+1}) для всех последующмх состояний.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_action = policy_net(non_final_next_states).detach().max(1)[1].view(-1,1)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).detach().gather(1, next_state_action).view(-1) # берем значение максимума\n",
    "    \n",
    "    # Считаем ожидаемое значение функции оценки ценности действия  Q-values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Считаем ошибку Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "    # Оптимизация модели\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    \n",
    "    del non_final_mask\n",
    "    del next_state_action\n",
    "    del non_final_next_states\n",
    "    del state_batch\n",
    "    del action_batch\n",
    "    del reward_batch\n",
    "    del state_action_values\n",
    "    del next_state_values\n",
    "    del expected_state_action_values\n",
    "    del loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# количество эпизодов, которые играем\n",
    "NUM_EPISODES = 2000\n",
    "\n",
    "# количество кадров, между которыми обучаем модель\n",
    "OPTIMIZE_MODEL_STEP = 4\n",
    "# количество кадров, между которыми обновляем target-модель\n",
    "TARGET_UPDATE=10000\n",
    "\n",
    "# несколько шагов для разогрева модели()\n",
    "STEPS_BEFORE_TRAIN = 50000\n",
    "\n",
    "# параметры для e-greedy стратегии выбора действия\n",
    "EPS_START = 1\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = 1000000 \n",
    "eps_threshold = EPS_START\n",
    "policy_net.train()\n",
    "target_net.eval()\n",
    "state_holder = StateHolder()\n",
    "\n",
    "memory = ReplayMemory()\n",
    "test_rewards = []\n",
    "train_rewards = []\n",
    "\n",
    "# Общее число \n",
    "steps_done = 0\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задание 3. Заполнить пропуски в нижеприведенном коде.\n",
    "#optimizer = optim.Adam(policy_net.parameters(), lr=0.000001)\n",
    "NUM_EPISODES = 10000\n",
    "for e in tqdm.tqdm_notebook(range(NUM_EPISODES)):\n",
    "    env.reset()\n",
    "    lives = 5\n",
    "    ep_rewards = []\n",
    "    state_holder.push(get_screen())\n",
    "    \n",
    "    for t in count():\n",
    "        state = state_holder.get()\n",
    "        # Рассчет eps_threshold'а для e-greedy\n",
    "        eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "        action = select_action(state, eps_threshold)\n",
    "        steps_done += 1\n",
    "        \n",
    "        # Шаг одного кадра игры \n",
    "        _, reward, done, info = env.step(action.item())\n",
    "        life = info['ale.lives']\n",
    "        ep_rewards.append(reward)\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        # где:\n",
    "        # reward - награда, полученная в рамках последнего действия\n",
    "        # done - флаг окончания эпизода\n",
    "        # info - важная системная информация\n",
    "        state_holder.push(get_screen())\n",
    "        next_state = state_holder.get()\n",
    "        # Код обработки перехода\n",
    "        # Работа с ReplayMemory\n",
    "        if not done:\n",
    "            new_reward = reward\n",
    "            next_state, lives = (next_state.to('cpu'), lives)\n",
    "            memory.add_experience(action, next_state[-1,-1].to('cpu'), new_reward, done)\n",
    "            state = next_state\n",
    "        else:\n",
    "            new_reward = torch.zeros_like(reward)\n",
    "            memory.add_experience(action, next_state[-1,-1].to('cpu'), new_reward, done)\n",
    "            state = next_state\n",
    "        \n",
    "        # Задание 3.1. Необходимо дополниь код следующим трюком, который значительно\n",
    "        # улучшает сходимость обучения. В случае, если эпизод не закончился, но агент на \n",
    "        # очередном шаге потерял жизнь, то такой переход надо класть в ReplayMemory как финальный.\n",
    "        # При этом, необходимо далее продолжать эпизод игры, пока не получите done == True\n",
    "        # Тет самым вы научите агента понимать, что терять жизни по дороге - плохо.\n",
    "        \n",
    "        # Шаг оптимизации\n",
    "        if (steps_done > STEPS_BEFORE_TRAIN) and steps_done % OPTIMIZE_MODEL_STEP == 0:\n",
    "            BATCH_SIZE = 32\n",
    "            optimize_model()\n",
    "        if e % 100 == 99 and  t == 0:\n",
    "            print('\\neps_threshold:', eps_threshold)\n",
    "            print('steps_done: ', steps_done)\n",
    "            # Средний reward по последним 100 эпизодам\n",
    "            print(\"Mean score : {}\".format(np.mean(train_rewards[-100:])))\n",
    "        if e % 10 == 9 and  t == 0:\n",
    "            print(\"10 ep.mean score : {}\".format(np.mean(train_rewards[-10:])))\n",
    "        # Шаг обновления target'сети\n",
    "        if steps_done % TARGET_UPDATE == 0:\n",
    "            print(\"Target net updated!\")\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "        # Код завершающий эпизод игры/обучения\n",
    "        if done:\n",
    "            train_rewards.append(np.sum(ep_rewards))\n",
    "            break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
